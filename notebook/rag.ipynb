{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43709455-67e3-49a0-88c0-914748ba987f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c2f7c26-c359-4257-8344-892403def72e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "941dbe31-e8bf-4b54-b30d-8ec4774d9afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rpath = os.path.abspath('/home/user/Documents/10/w7/PromptBuilder for RAG/backend')\n",
    "\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abd574b8-d9c9-4d40-894d-3d47fcb4a3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from service.file_processor import FileProcessor\n",
    "from service.chroma_db_manager import ChromaDBManager\n",
    "from service.rag_processor import RAGProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6d14894-4f23-456b-b01a-f7405135ca61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "582e9991-7aa7-402a-b7d1-7af7f3c2ffbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a838e53b-1ec9-4823-b685-affabc99f1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"),model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ff82b83-f08e-4ae9-b6b9-7dff5b6466b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filePath = os.getenv(\"PATH_TO_TXT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f05de80-c197-432c-aa9d-4af0114e2466",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load and split the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7174f47-8f00-42a7-8c98-23edbedf8eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Secondhand cars left us hitchhiking on the side of the freeway. A secondhand car was also the reason my mom got married. If it hadn’t been for the Volkswagen that didn’t work, we never would have looked for the mechanic who became the husband who became the stepfather who became the man who tortured us for years and put a bullet in the back of my mother’s head—I’ll take the new car with the warranty every time.', metadata={'source': '../data/data.txt'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = FileProcessor(filePath)\n",
    "file.load_file()\n",
    "sp = file.split_file()\n",
    "sp[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be830252-10f5-4a64-8ebc-388202207c56",
   "metadata": {},
   "source": [
    "# Create and store it to chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "790b2a1f-6fb7-4ea9-9d62-8427f101e2be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x7aedcacd2dd0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma = ChromaDBManager()\n",
    "chromaDb= chroma.store_and_load_chroma_db(sp)\n",
    "chromaDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17bd7c40-8d2f-467f-a932-36218c2ada60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='My great-grandmother lived with us as well. We called her Koko. She was super old, well into her nineties, stooped and frail, completely blind. Her eyes had gone white, clouded over by cataracts. She couldn’t walk without someone holding her up. She’d sit in the kitchen next to the coal stove, bundled up in long skirts and head scarves, blankets over her', metadata={'source': '../data/data.txt'}),\n",
       "  0.2524379789829254),\n",
       " (Document(page_content='My grandmother Frances Noah was the family matriarch. She ran the house, looked after the kids, did the cooking and the cleaning. She’s barely five feet tall, hunched over from years in the factory, but rock hard and still to this day very active and very much alive. Where my grandfather was big and boisterous, my grandmother was calm, calculating, with a mind as sharp as anything. If you need to know anything in the family history, going back to the 1930s, she can tell you what day it happened, where it happened, and why it happened. She remembers it all.', metadata={'source': '../data/data.txt'}),\n",
       "  0.2786509096622467),\n",
       " (Document(page_content='When my mom turned twenty-one, her aunt fell ill and that family could no longer keep her in Transkei. My mom wrote to my gran, asking her to send the price of a train ticket, about thirty rand, to bring her home. Back in Soweto, my mom enrolled in the secretarial course that allowed her to grab hold of the bottom rung of the white-collar world. She worked and worked and worked but, living under my grandmother’s roof, she wasn’t allowed to keep her own wages. As a secretary, my mom was bringing home more money than anyone else, and my grandmother insisted it all go to the family. The\\nfamily needed a radio, an oven, a refrigerator, and it was now my mom’s job to provide it.', metadata={'source': '../data/data.txt'}),\n",
       "  0.3186028003692627),\n",
       " (Document(page_content='combination right there. Whenever I prayed, my grandmother would say, “That prayer is going to get answered. I can feel it.”\\nWomen in the township always had something to pray for—money problems, a son who’d been arrested, a daughter who was sick, a husband who drank. Whenever the prayer meetings were at our house, because my prayers were so good, my grandmother would want me to pray for everyone. She would turn to me and say, “Trevor, pray.” And I’d pray. I loved doing it. My grandmother had convinced me that my prayers got answered. I felt like I was helping people.\\n—', metadata={'source': '../data/data.txt'}),\n",
       "  0.3391699194908142)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma.search_chroma_db_with_score(\"grandmother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d100bb9f-202a-4c8e-964f-063ce88bb682",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "137caa53-cf76-4520-b092-9c51b4a044e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7aedcacd2dd0>, search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.5})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_processor = RAGProcessor(llm)\n",
    "rag_processor.create_rag(chromaDb)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c082052-68d1-4f52-85bc-5daf1268846a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(itemgetter('question'))\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7aedcacd2dd0>, search_type='similarity_score_threshold', search_kwargs={'score_threshold': 0.5}),\n",
       "  question: RunnableLambda(itemgetter('question'))\n",
       "}\n",
       "| RunnableAssign(mapper={\n",
       "    context: RunnableLambda(itemgetter('context'))\n",
       "  })\n",
       "| {\n",
       "    response: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"\\n            You are an expert LLM prompt writing service.\\n            You take their prompt as input and output a better prompt based on your prompt writing expertise and the knowledge on the context. \\n            You must write 5 top prompts that can achieve their desired objective and expected outputs, please respond with 'I don't know':\\n            {question}\\n            Context:\\n            {context}\\n            \"))])\n",
       "              | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7aedcd6d8f90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7aedcada2bd0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''),\n",
       "    context: RunnableLambda(itemgetter('context'))\n",
       "  }"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the chain with the templet \n",
    "chain = rag_processor.process_rag_chain() \n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "880d1d90-6a4a-4e95-b669-0bb6a6d8c04d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Explore the unique relationship between Koko and the narrator, delving into the dynamics of communication and companionship despite physical limitations.\\n2. Reflect on the significance of Koko's presence in the household, considering her role as a silent yet integral member of the family.\\n3. Discuss the impact of Koko's immobility on the narrator's perception of her, highlighting the complexities of human connection beyond physical interaction.\\n4. Examine the daily routines and interactions involving Koko, shedding light on the moments of tenderness and routine that define their shared experiences.\\n5. Analyze the symbolism of Koko's constant presence by the coal stove, drawing parallels between her warmth and the emotional comfort she provides to those around her.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"tell me about Koko\"\n",
    "\n",
    "reault= rag_processor.invoker(prompt)\n",
    "reault[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e904ef-3cd4-4c49-93a1-01d9efeac411",
   "metadata": {},
   "source": [
    "# EVALUATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a323b79-0222-42a7-b1fb-dfcc3649c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c816df5b-f194-4dc5-a559-9f4d36bc3ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = FileProcessor(filePath)\n",
    "file.load_file()\n",
    "documents = file.split_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17086a69-8060-4da8-a860-10d2d7232794",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/1974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e349ae8b391485d89d539199c7dbfe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/executor.py\", line 95, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/asyncio/base_events.py\", line 653, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/executor.py\", line 83, in _aresults\n",
      "    raise e\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/testset/evolutions.py\", line 142, in evolve\n",
      "    ) = await self._aevolve(current_tries, current_nodes)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/testset/evolutions.py\", line 466, in _aevolve\n",
      "    simple_question, current_nodes, _ = await self.se._aevolve(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/testset/evolutions.py\", line 296, in _aevolve\n",
      "    passed = await self.node_filter.filter(merged_node)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/testset/filters.py\", line 56, in filter\n",
      "    results = await self.llm.generate(prompt=prompt)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 390, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.PermissionDeniedError: Error code: 403 - {'error': {'message': 'Project `proj_lrxkI4g7m9iBm666PLebUMRA` does not have access to model `gpt-4`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m      5\u001b[0m generator \u001b[38;5;241m=\u001b[39m TestsetGenerator\u001b[38;5;241m.\u001b[39mfrom_langchain(\n\u001b[1;32m      6\u001b[0m     generator_llm,\n\u001b[1;32m      7\u001b[0m     critic_llm,\n\u001b[1;32m      8\u001b[0m     embeddings\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m testset \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mgenerate_with_langchain_docs(documents, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, distributions\u001b[38;5;241m=\u001b[39m{simple: \u001b[38;5;241m0.5\u001b[39m, reasoning: \u001b[38;5;241m0.25\u001b[39m, multi_context: \u001b[38;5;241m0.25\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ragas/testset/generator.py:210\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[0;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39madd_documents(\n\u001b[1;32m    207\u001b[0m     [Document\u001b[38;5;241m.\u001b[39mfrom_langchain_document(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    208\u001b[0m )\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    211\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[1;32m    212\u001b[0m     distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[1;32m    213\u001b[0m     with_debugging_logs\u001b[38;5;241m=\u001b[39mwith_debugging_logs,\n\u001b[1;32m    214\u001b[0m     is_async\u001b[38;5;241m=\u001b[39mis_async,\n\u001b[1;32m    215\u001b[0m     raise_exceptions\u001b[38;5;241m=\u001b[39mraise_exceptions,\n\u001b[1;32m    216\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    217\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ragas/testset/generator.py:305\u001b[0m, in \u001b[0;36mTestsetGenerator.generate\u001b[0;34m(self, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    303\u001b[0m     test_data_rows \u001b[38;5;241m=\u001b[39m exec\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m test_data_rows:\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=3, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89d5fb9b-97c7-4c4d-be4a-ea2b0b05cd7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116957/1616921231.py:1: DeprecationWarning: The function with_openai was deprecated in 0.1.4, and will be removed in the 0.2.0 release. Use from_langchain instead.\n",
      "  generator = TestsetGenerator.with_openai()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/1974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350a40ede9664d52b8a654d8d50f92b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/executor.py\", line 95, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/asyncio/base_events.py\", line 653, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/executor.py\", line 83, in _aresults\n",
      "    raise e\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/testset/evolutions.py\", line 142, in evolve\n",
      "    ) = await self._aevolve(current_tries, current_nodes)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/testset/evolutions.py\", line 551, in _aevolve\n",
      "    result = await self._acomplex_evolution(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/testset/evolutions.py\", line 379, in _acomplex_evolution\n",
      "    simple_question, current_nodes, _ = await self.se._aevolve(\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/testset/evolutions.py\", line 296, in _aevolve\n",
      "    passed = await self.node_filter.filter(merged_node)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/testset/filters.py\", line 56, in filter\n",
      "    results = await self.llm.generate(prompt=prompt)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py\", line 390, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.PermissionDeniedError: Error code: 403 - {'error': {'message': 'Project `proj_lrxkI4g7m9iBm666PLebUMRA` does not have access to model `gpt-4`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m generator \u001b[38;5;241m=\u001b[39m TestsetGenerator\u001b[38;5;241m.\u001b[39mwith_openai()\n\u001b[0;32m----> 3\u001b[0m testset \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mgenerate_with_langchain_docs(documents, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, distributions\u001b[38;5;241m=\u001b[39m{simple: \u001b[38;5;241m0.5\u001b[39m, reasoning: \u001b[38;5;241m0.25\u001b[39m, multi_context: \u001b[38;5;241m0.25\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ragas/testset/generator.py:210\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[0;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39madd_documents(\n\u001b[1;32m    207\u001b[0m     [Document\u001b[38;5;241m.\u001b[39mfrom_langchain_document(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    208\u001b[0m )\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    211\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[1;32m    212\u001b[0m     distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[1;32m    213\u001b[0m     with_debugging_logs\u001b[38;5;241m=\u001b[39mwith_debugging_logs,\n\u001b[1;32m    214\u001b[0m     is_async\u001b[38;5;241m=\u001b[39mis_async,\n\u001b[1;32m    215\u001b[0m     raise_exceptions\u001b[38;5;241m=\u001b[39mraise_exceptions,\n\u001b[1;32m    216\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    217\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ragas/testset/generator.py:305\u001b[0m, in \u001b[0;36mTestsetGenerator.generate\u001b[0;34m(self, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    303\u001b[0m     test_data_rows \u001b[38;5;241m=\u001b[39m exec\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m test_data_rows:\n\u001b[0;32m--> 305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "generator = TestsetGenerator.with_openai()\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425e06a-a3fd-4f50-ba23-b3644d4ca004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-18056' coro=<AsyncClient.aclose() done, defined at /home/user/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1996> exception=RuntimeError('Event loop is closed')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpx/_client.py\", line 2003, in aclose\n",
      "    await self._transport.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py\", line 383, in aclose\n",
      "    await self._pool.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 324, in aclose\n",
      "    await connection.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
      "    await self._connection.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 253, in aclose\n",
      "    await self._network_stream.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpcore/_backends/anyio.py\", line 54, in aclose\n",
      "    await self._stream.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/anyio/streams/tls.py\", line 202, in aclose\n",
      "    await self.transport_stream.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1181, in aclose\n",
      "    self._transport.close()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/asyncio/selector_events.py\", line 860, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"/home/user/anaconda3/lib/python3.11/asyncio/base_events.py\", line 761, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/asyncio/base_events.py\", line 519, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-18058' coro=<AsyncClient.aclose() done, defined at /home/user/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1996> exception=RuntimeError('Event loop is closed')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpx/_client.py\", line 2003, in aclose\n",
      "    await self._transport.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py\", line 383, in aclose\n",
      "    await self._pool.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 324, in aclose\n",
      "    await connection.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 173, in aclose\n",
      "    await self._connection.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 253, in aclose\n",
      "    await self._network_stream.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/httpcore/_backends/anyio.py\", line 54, in aclose\n",
      "    await self._stream.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/anyio/streams/tls.py\", line 202, in aclose\n",
      "    await self.transport_stream.aclose()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1181, in aclose\n",
      "    self._transport.close()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/asyncio/selector_events.py\", line 860, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"/home/user/anaconda3/lib/python3.11/asyncio/base_events.py\", line 761, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"/home/user/anaconda3/lib/python3.11/asyncio/base_events.py\", line 519, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n"
     ]
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f9b1c-ee9a-4b96-b589-25fb843211d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = testset.to_pandas()\n",
    "\n",
    "test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66cebc86-d76e-44ad-a79c-423baf5fe838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m contexts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m test_questions:\n\u001b[1;32m      5\u001b[0m   response \u001b[38;5;241m=\u001b[39mrag_processor\u001b[38;5;241m.\u001b[39minvoker(prompt)\n\u001b[1;32m      6\u001b[0m   answers\u001b[38;5;241m.\u001b[39mappend(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_questions' is not defined"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "for question in test_questions:\n",
    "  response =rag_processor.invoker(prompt)\n",
    "  answers.append(response[\"response\"].content)\n",
    "  contexts.append([context.page_content for context in response[\"context\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fffbc22-7788-41bd-85d5-3e5f4c4fc9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      3\u001b[0m response_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict({\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m : test_questions,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m : answers,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m : contexts,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m : test_groundtruths\n\u001b[1;32m      8\u001b[0m })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_questions' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"question\" : test_questions,\n",
    "    \"answer\" : answers,\n",
    "    \"contexts\" : contexts,\n",
    "    \"ground_truth\" : test_groundtruths\n",
    "})\n",
    "response_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0e970-0454-49de-9383-4dac09688fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "]\n",
    "\n",
    "results = evaluate(response_dataset, metrics)\n",
    "\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447dc478-f85e-4eb7-8f80-1be4fb3b0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.to_pandas()\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
